# GEMINI.md

This file provides instructional context about the Crop Analysis System project.

## Project Overview

This project is a Django-based web application designed to manage and execute a crop analysis pipeline. The system allows users to launch data processing jobs on satellite imagery (`.tif` files) to identify and analyze crop growth. It provides a web interface for creating, monitoring, and inspecting these jobs and their outputs.

The core of the system is an asynchronous pipeline built with Celery, which performs a sequence of operations:

1.  **Inference**: A deep learning model (a TransUNet, located in the `TransUNet/` directory) is used to perform inference on input image tiles, generating crop masks.
2.  **Merge**: The resulting tiles are merged into a single, large crop mask for each administrative region and crop type.
3.  **Area Calculation**: The total area for each crop is calculated from the merged masks and corresponding shapefiles.

### Key Technologies

*   **Backend**: Django, Django REST Framework
*   **Asynchronous Task Queue**: Celery with Redis as the message broker.
*   **Frontend**: Django Templates with vanilla JavaScript for dynamic UI elements.
*   **Database**: SQLite (default configuration).
*   **ML Model**: A PyTorch-based TransUNet for semantic segmentation.

### Architecture

*   **`config/`**: Contains the main Django project configuration, including `settings.py`, root `urls.py`, and `celery.py`.
*   **`core/`**: A Django app that defines the data models (`Job`, `PipelineConfig`, etc.), manages the web interface (views and templates), and handles job creation logic.
*   **`pipeline/`**: A Django app that contains the Celery tasks (`tasks.py`) and the core data processing services for inference, merging, and area calculation.
*   **`TransUNet/`**: Contains the source code for the neural network model used in the inference step.
*   **`media/`**: The primary directory for all data, including:
    *   `input/`: Source satellite imagery.
    *   `output/`: Results generated by the pipeline jobs.
    *   `weights/`: Saved model weights for the neural network.
    *   `layers/`: Shapefiles used for area calculation.

## Building and Running

While explicit dependency and startup scripts are not present, the following steps can be inferred for running the application.

### Dependencies

A `requirements.txt` file is not available. Based on the source code, the main dependencies are:

*   `django`
*   `djangorestframework`
*   `celery`
*   `redis`
*   `python-dotenv`
*   `whitenoise`

Additional libraries for data processing are required for the pipeline to function, likely including:
*   `torch` and `torchvision`
*   Geospatial libraries like `gdal`, `rasterio`, and `geopandas`.


### Running the Application

1.  **Start Redis Server**: The Celery message broker must be running.
    ```sh
    redis-server
    ```

2.  **Run the Django Web Server**: This starts the application's web interface, accessible at `http://127.0.0.1:8000`.
    ```sh
    python manage.py runserver
    ```

3.  **Run the Celery Worker**: This worker process will execute the pipeline tasks in the background.
    ```sh
    celery -A config worker -l info
    ```

### Running Tests

There are no tests included in the project.

**TODO**: Create a testing suite to verify the functionality of the pipeline services and web interface.

## Development Conventions

*   **Configuration**: Environment variables are managed via a `.env` file and loaded in `config/settings.py`. Key paths like `MEDIA_ROOT` and Redis connection details are set here.
*   **Data Models**: The `core/models.py` file defines the database schema, including `Job`, `PipelineConfig`, and `Country`. The database is the source of truth for job status and configuration.
*   **Asynchronous Tasks**: Heavy processing is offloaded to Celery tasks defined in `pipeline/tasks.py`. Tasks are chained together (`inference_task` -> `merge_task` -> `area_task`) to create the full workflow.
*   **File Management**: The system relies on a strict directory structure within the `media/` folder for inputs and outputs. Paths are managed and resolved using logic in `core/utils/app_settings.py` and `pipeline/services/common.py`.
*   **Frontend**: The user interface is rendered using Django templates. Client-side interactions (like fetching data for dropdowns or polling job status) are handled with JavaScript in the `static/js/` directory, which makes API calls to the Django backend.


### Code Writing Rules

#### 1) Python Style & Quality
- Use Python 3.10+.
- Follow PEP8 by default, but formatter/linter output is the source of truth.
- Every function/class should include a short meaningful docstring.
- Prefer maintainable code over “just working” code.

#### 2) Formatting / Linting / Typing (Recommended Tools)
- Formatter: `black`
- Import sorting: `isort`
- Linter: `ruff` (or `flake8`)
- Type checking: `mypy` (gradual adoption is fine)
- Type hints are required for new core modules and critical functions.

#### 3) Directory / Module Structure
- Any cross-app logic must be moved into a service layer (`pipeline/services/` or shared modules).
- Views/Serializers should focus on I/O and avoid embedding business logic.
- Celery tasks should handle orchestration (workflow chaining) and delegate heavy logic to service functions.

#### 4) Error Handling
- Keep try/except blocks small and never silently swallow exceptions.
- On failure, always log enough context and provide a clear error message.
- Convert common failures (file/path/model loading) into user-friendly messages when possible.

#### 5) Logging
- Do not use `print()`; use Django logger or Python `logging`.
- Logs should include:
  - Job ID / Region / Crop Type
  - Input paths or key parameters
  - Processing stage (inference/merge/area)
- Never log sensitive or private data.

#### 6) File / Path Handling
- Avoid hardcoded strings for paths; use `pathlib.Path` or project path utilities.
- Do not break the `media/` structure; always ensure output directories exist.
- Consider memory usage when handling large files.

#### 7) Data / DB Rules
- Job status updates must be handled in a single-responsibility function/method.
- Avoid partial-success states; on failure, always transition to FAILED clearly.
- Fetch only required fields and avoid N+1 query patterns.

#### 8) Performance & Reliability
- When processing large `.tif` files, consider parallelization and I/O bottlenecks.
- Celery tasks should be retryable and safe to run multiple times (idempotent).
- Structure intermediate outputs so they can be reused when appropriate.